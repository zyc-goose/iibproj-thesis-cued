%!TEX root = ../thesis.tex

%%%%% Chapter: Software Setup %%%%%
\chapter{Evaluation Results}
\label{chap:eval-results}

\ifpdf
    \graphicspath{{Chapter9/Figs/Raster/}{Chapter9/Figs/PDF/}{Chapter9/Figs/}}
\else
    \graphicspath{{Chapter9/Figs/Vector/}{Chapter9/Figs/}}
\fi


\section{Overview}

\begin{table}[!htb]
    \caption{Adjustable System Parameters}
    \centering
    \label{tab:sys-param-list}
    \begin{tabular}{l l c}
    \toprule
    Name & Description & Range \\
    \midrule
    OCR-scale & Scale of the OCR PLA & \{`word', `line', `paragraph', `page'\} \\
    OCR-model & OCR model used for recognition & \{`\texttt{eng}', `\texttt{eng-9000}'\} \\
    JWF-$\sigma_t$ & Gaussian STD for time constraint & (0, $\infty$) \\
    JWF-$\alpha_c$ & Penalty for common words & (0, 1]\\
    \bottomrule
    \end{tabular}
\end{table}

A number of parameters in the system can be adjusted. \Cref{tab:sys-param-list} lists all the adjustable system parameters. Evaluation can be performed on the system with various sets of parameters.

Due to the time limitation and the high complexity of labelling the ground-truth data for lectures, only a single set of ground-truth data has been labelled for the system evaluation. The labelled ground-truth data corresponds to the first hour-long lecture of Part IA Math (2010) taught by Prof. Malcolm Smith.

The methodology of evaluation is simple: starting from a set of baseline parameters, we try to adjust each parameter individually in turn and observe the effects. Then we combine the individual best parameters together to see the final results.

\section{The Baseline System}

The set of parameters of the baseline system is defined as:
\begin{equation*}
    \begin{cases}
        \text{OCR-scale} &=\quad \text{`paragraph'} \\
        \text{OCR-model} &=\quad \text{`\texttt{eng}'} \\
        \text{JWF-}\sigma_t &=\quad \infty \\
        \text{JWF-}\alpha_c &=\quad 1
    \end{cases}
\end{equation*}
which provides a starting points for the system evaluation. \Cref{tab:eval-res-baseline} shows the initial evaluation results for the baseline system.

\begin{table}[!tb]
    \caption{Evaluation results for the baseline system}
    \centering
    \label{tab:eval-res-baseline}
    \begin{tabular}{l c c c c c c c}
    \toprule
    \multirow{2}{*}{System} &
    \multirow{2}{*}{TPR}& \multirow{2}{*}{TNR} & \multicolumn{4}{c}{OCR-WER} & \multirow{2}{*}{Align-Recall} \\
    \cmidrule{4-7}
    & & & All & I & D & S & \\
    \midrule
    \textbf{Baseline} &
    90.04\% & 85.94\% & 48.15\% & 8.64\% &9.88\% &29.63\% &43.67\% \\
    \bottomrule
    \end{tabular}
\end{table}

\section{Changing the OCR Scale}

\begin{table}[!b]
    \caption{Effect of changing OCR scale}
    \centering
    \label{tab:eval-res-scale}
    \begin{tabular}{l c c c c}
    \toprule
    \multirow{2}{*}{Evaluation Metric}& \multicolumn{4}{c}{OCR-scale}\\
    \cmidrule{2-5}
    & word & line & paragraph & page \\
    \midrule
    OCR-TPR & 51.71\%&83.60\%&90.04\%&100.0\%\\
    OCR-TNR & 98.09\%&92.71\%&85.94\%&1.51\%\\
    Align-Recall&7.24\%&33.17\%&43.67\%&82.85\%\\
    \bottomrule
    \end{tabular}
\end{table}

Changing the OCR scale affects all the metrics except the WER. \Cref{tab:eval-res-scale} lists the evaluation results for the 4 different OCR scales. Clearly a tradeoff exists when changing the OCR scale: coarse scales have high alignment accuracy and TPR but very low TNR; fine scales have low alignment accuracy and TPR but high TNR.

In practice, extremely coarse scales like `page' won't be useful for students who want to find the audio segments correspond to a short sentence, although these scales have very high alignment accuracy. Therefore, we actually have a tradeoff between the spatial resolution with respect to the handouts and the accuracy of alignment. The spatial resolution is partially reflected by the TNR.

\section{Changing the OCR Model}

\begin{table}[!tb]
    \caption{Effect of changing OCR model}
    \centering
    \label{tab:eval-res-ocr-model}
    \begin{tabular}{l c c c c c}
    \toprule
    \multirow{2}{*}{OCR-model} &\multicolumn{4}{c}{OCR-WER} & \multirow{2}{*}{Align-Recall} \\
    \cmidrule{2-5}
     & All & I & D & S & \\
    \midrule
    \texttt{eng} & 48.15\%&8.64\%&9.88\%&29.63\%&43.67\%\\
    \texttt{eng-9000} & 52.22\%&11.98\%&8.52\%&31.73\%&43.14\%\\
    \bottomrule
    \end{tabular}
\end{table}

Switching between OCR models won't affect TPR and TNR, but will affect the rest of the evaluation metrics. \Cref{tab:eval-res-ocr-model} shows the evaluation results for the two OCR models \texttt{eng} and \texttt{eng-9000}. Unfortunately the fine-tuned model \texttt{eng-9000} performs worse in both OCR-WER and the alignment recall rate.

A possible reason for this is that the improvement of the fine-tuned model on handwritten text recognition is limited while its accuracy drop in recognising printed text has a significant impact on the OCR-WER and the eventual alignment recall rate. This also shows that the idea of improving the accuracy on handwritten text by fine-tuning the Tesseract model is hard and needs further investigation.

\section{Changing the Time Constraint Parameter}

\begin{table}[!b]
    \caption{Effect of changing Gaussian STD $\sigma_t$}
    \centering
    \label{tab:eval-res-gauss}
    \begin{tabular}{l c c c c c}
    \toprule
    \multirow{2}{*}{Evaluation Metric} & \multicolumn{5}{c}{Gaussian STD $\sigma_t$} \\
    \cmidrule{2-6}
    & $\sigma_t$ = 0.01 & $\sigma_t$ = 1 & $\sigma_t$ = 5 & $\sigma_t$ = 100 & $\sigma_t = \infty$\\
    \midrule
    Align-Recall & 28.54\%&34.06\%&45.15\%&45.15\%&\textbf{43.67\%}\\
    \bottomrule
    \end{tabular}
\end{table}

As explained in \Cref{subsec:time-constraint}, the Gaussian standard deviation $\sigma_t$ is used to penalise large time differences in the alignment of sequences. The smaller the $\sigma_t$ (small STD value narrows the width of Gaussian), the stronger the penalty effect.

The value of $\sigma_t$ only affects the alignment recall rate. \Cref{tab:eval-res-gauss} lists the results for different values of $\sigma_t$. Clearly we can see that too much penalty on time differences leads to poor alignment accuracy, and suitable values like $\sigma_t$ = 5 could improve the accuracy by a small amount. Also we shall notice that the accuracy converges to 45.15\% for larger finite values of $\sigma_t$, which may result from the convergence of the optimal alignment $Z_{opt}$.


\section{Changing the Common-word Penalising Parameter}

\begin{table}[!t]
    \caption{Effect of changing the common-word penalising parameter $\alpha_c$}
    \centering
    \label{tab:eval-res-alpha}
    \begin{tabular}{l c c c c c}
    \toprule
    \multirow{2}{*}{Evaluation Metric} & \multicolumn{5}{c}{Common-word penalising parameter $\alpha_c$} \\
    \cmidrule{2-6}
    & $\alpha_c$ = 0.001 & $\alpha_c$ = 0.1 & $\alpha_c$ = 0.5 & $\alpha_c$ = 0.9 & $\alpha_c$ = 1\\
    \midrule
    Align-Recall & 44.14\%&44.14\%&44.50\%&44.41\%&\textbf{43.67\%}\\
    \bottomrule
    \end{tabular}
\end{table}

As explained earlier in \Cref{subsec:common-word-penalty}, the final JWF used in the system scales the weights of common-word matches by a factor $\alpha_c$ (smaller than 1), which essentially penalises matches of simple words like `a' or `the'. 

The results of varying $\alpha_c$ are tabulated in \Cref{tab:eval-res-alpha}. The penalising factor $\alpha_c$ generally improves the alignment accuracy by a small amount compared to the baseline case $\alpha_c$ = 1, with the optimal accuracy achieved at roughly $\alpha_c$ = 0.5.

\section{Combining the Best Parameters}

Now we collect the best parameters together:
\begin{equation*}
    \begin{cases}
        \text{OCR-scale} &=\quad \text{`paragraph'} \\
        \text{OCR-model} &=\quad \text{`\texttt{eng}'} \\
        \text{JWF-}\sigma_t &=\quad 5 \\
        \text{JWF-}\alpha_c &=\quad 0.5
    \end{cases}
\end{equation*}
Notice that we have only updated $\sigma_t$ and $\alpha_c$ compared to the baseline parameters, and the set of the best parameters will only change the value of the alignment recall rate. The set of best parameters achieves an alignment recall rate of 46.67\%, compared to the baseline 43.67\%.

\nomenclature[z-STD]{STD}{Standard Deviation}